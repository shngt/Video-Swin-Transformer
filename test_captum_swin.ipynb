{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from mmaction.datasets.pipelines import Compose\n",
    "import mmcv\n",
    "\n",
    "# ..........torch imports............\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#.... Captum imports..................\n",
    "from captum.attr import LayerGradientXActivation, LayerIntegratedGradients\n",
    "\n",
    "from captum.concept import TCAV\n",
    "from captum.concept import Concept\n",
    "\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset\n",
    "from captum.concept._utils.common import concepts_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/.conda/envs/video-swin/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642814471/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import init_recognizer, inference_recognizer\n",
    "\n",
    "config_file = 'configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py'\n",
    "config = mmcv.Config.fromfile(config_file)\n",
    "device = 'cpu' # or 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "model = init_recognizer(config_file, device=device, checkpoint='checkpoints/swin_tiny_patch244_window877_kinetics400_1k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "    \n",
    "model.backbone.register_forward_hook(get_activation('cls_head.avg_pool'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensors)\n",
    "print(activation['backbone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_133113/178861805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls_head.avg_pool'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'activation' is not defined"
     ]
    }
   ],
   "source": [
    "activation['cls_head.avg_pool'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = activation['backbone']\n",
    "y = torch.mean(x, axis=0).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 16, 7, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to normalize a video to Kinetics-400 mean and standard deviation\n",
    "def transform(video, cfg=config):\n",
    "    # build the data pipeline\n",
    "    test_pipeline = cfg.data.test.pipeline\n",
    "    data = dict(filename=video, label=-1, start_index=0, modality='RGB')\n",
    "    if 'Init' not in test_pipeline[0]['type']:\n",
    "        test_pipeline = [dict(type='OpenCVInit')] + test_pipeline\n",
    "    else:\n",
    "        test_pipeline[0] = dict(type='OpenCVInit')\n",
    "    for i in range(len(test_pipeline)):\n",
    "        if 'Decode' in test_pipeline[i]['type']:\n",
    "            test_pipeline[i] = dict(type='OpenCVDecode')\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    data = test_pipeline(data)\n",
    "    data = data['imgs'].to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"../../shashank/kinetics-dataset/k400/avinab/interest_classes/\"):\n",
    "    concept_path = os.path.join(concepts_path, name) + \"/\"\n",
    "    dataset = CustomIterableDataset(transform, concept_path)\n",
    "    concept_iter = dataset_to_dataloader(dataset)\n",
    "\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_path = \"../data/sravan_concepts/k400_concept_videos/\"\n",
    "\n",
    "fork_concept = assemble_concept(\"fork\", 0, concepts_path=concepts_path)\n",
    "random_concept = assemble_concept(\"random\", 2, concepts_path=concepts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/.conda/envs/video-swin/lib/python3.7/site-packages/captum/concept/_utils/classifier.py:131: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  \"Using default classifier for TCAV which keeps input\"\n"
     ]
    }
   ],
   "source": [
    "layers=['backbone']\n",
    "\n",
    "mytcav = TCAV(model=model,\n",
    "              layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Concept(0, 'fork'), Concept(2, 'random')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimental_set_rand = [[fork_concept, random_concept]]\n",
    "experimental_set_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images from folder\n",
    "input_tensors = torch.stack([transform(video) for video in glob.glob(\"../../shashank/kinetics-dataset/k400/avinab/interest_classes/dining/*.mp4\")])\n",
    "#input_tensors = torch.randn([1, 24, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98, 12, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "target_ind = 91\n",
    "\n",
    "tcav_scores_w_random = mytcav.interpret(inputs=input_tensors,\n",
    "                                        experimental_sets=experimental_set_rand,\n",
    "                                        target=target_ind\n",
    "                                       )\n",
    "tcav_scores_w_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('video-swin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d1bbba01dc72694b6cd605560e76f8ea2d618c46f33953a079d06611bd192fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
