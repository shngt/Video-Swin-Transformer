{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from mmaction.datasets.pipelines import Compose\n",
    "import mmcv\n",
    "\n",
    "# ..........torch imports............\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#.... Captum imports..................\n",
    "from captum.attr import LayerGradientXActivation, LayerIntegratedGradients\n",
    "\n",
    "from captum.concept import TCAV\n",
    "from captum.concept import Concept\n",
    "\n",
    "from captum.concept._utils.data_iterator import dataset_to_dataloader, CustomIterableDataset\n",
    "from captum.concept._utils.common import concepts_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import init_recognizer, inference_recognizer\n",
    "\n",
    "config_file = 'configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py'\n",
    "config = mmcv.Config.fromfile(config_file)\n",
    "device = 'cpu' # or 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "model = init_recognizer(config_file, device=device, checkpoint='checkpoints/swin_tiny_patch244_window877_kinetics400_1k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63047/1773437291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backbone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'backbone'"
     ]
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "    \n",
    "model.backbone.register_forward_hook(get_activation('cls_head.avg_pool'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensors)\n",
    "print(activation['backbone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768, 16, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation['cls_head.avg_pool'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = activation['backbone']\n",
    "y = torch.mean(x, axis=0).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 16, 7, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to normalize a video to Kinetics-400 mean and standard deviation\n",
    "def transform(video, cfg=config):\n",
    "    # build the data pipeline\n",
    "    test_pipeline = cfg.data.test.pipeline\n",
    "    data = dict(filename=video, label=-1, start_index=0, modality='RGB')\n",
    "    if 'Init' not in test_pipeline[0]['type']:\n",
    "        test_pipeline = [dict(type='OpenCVInit')] + test_pipeline\n",
    "    else:\n",
    "        test_pipeline[0] = dict(type='OpenCVInit')\n",
    "    for i in range(len(test_pipeline)):\n",
    "        if 'Decode' in test_pipeline[i]['type']:\n",
    "            test_pipeline[i] = dict(type='OpenCVDecode')\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    data = test_pipeline(data)\n",
    "    data = data['imgs'].to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concepts_path=\"../../shashank/kinetics-dataset/k400/avinab/interest_classes/\"):\n",
    "    concept_path = os.path.join(concepts_path, name) + \"/\"\n",
    "    dataset = CustomIterableDataset(transform, concept_path)\n",
    "    concept_iter = dataset_to_dataloader(dataset)\n",
    "\n",
    "    return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_path = \"../../shashank/kinetics-dataset/k400/avinab/interest_classes/\"\n",
    "\n",
    "sample1_concept = assemble_concept(\"sample1\", 0, concepts_path=concepts_path)\n",
    "sample2_concept = assemble_concept(\"sample2\", 2, concepts_path=concepts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/.conda/envs/video-swin/lib/python3.7/site-packages/captum/concept/_utils/classifier.py:131: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  \"Using default classifier for TCAV which keeps input\"\n"
     ]
    }
   ],
   "source": [
    "layers=['backbone']\n",
    "\n",
    "mytcav = TCAV(model=model,\n",
    "              layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Concept(0, 'sample1'), Concept(2, 'sample2')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimental_set_rand = [[sample1_concept, sample2_concept]]\n",
    "experimental_set_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images from folder\n",
    "input_tensors = torch.stack([transform(video) for video in glob.glob(\"../../shashank/kinetics-dataset/k400/avinab/interest_classes/sample3/*.mp4\")])\n",
    "#input_tensors = torch.randn([1, 24, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n",
      "12 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/.conda/envs/video-swin/lib/python3.7/site-packages/captum/_utils/models/linear_model/train.py:351: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642814471/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  bias_values = torch.FloatTensor([sklearn_model.intercept_]).to(  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function captum.concept._core.tcav.TCAV.interpret.<locals>.<lambda>()>,\n",
       "            {'0-2': defaultdict(None,\n",
       "                         {'backbone': {'sign_count': tensor([1., 0.]),\n",
       "                           'magnitude': tensor([ 5.9711e-06, -5.9711e-06])}})})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ind = 71\n",
    "\n",
    "tcav_scores_w_random = mytcav.interpret(inputs=input_tensors,\n",
    "                                        experimental_sets=experimental_set_rand,\n",
    "                                        target=target_ind\n",
    "                                       )\n",
    "tcav_scores_w_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('video-swin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d1bbba01dc72694b6cd605560e76f8ea2d618c46f33953a079d06611bd192fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
